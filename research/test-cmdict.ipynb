{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastdtw in /Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages (0.3.4)\n",
      "Requirement already satisfied: numpy in /Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages (1.24.2)\n",
      "\u001b[33mWARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install fastdtw numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' Technically, it has a difficult transform how to communicate work and live, making our daily task more efficient when also very concerned about privacy and job replacement.', 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 8.0, 'text': ' Technically, it has a difficult transform how to communicate work and live, making our', 'tokens': [50364, 42494, 11, 309, 575, 257, 2252, 4088, 577, 281, 7890, 589, 293, 1621, 11, 1455, 527, 50764], 'temperature': 0.0, 'avg_logprob': -0.47807203020368305, 'compression_ratio': 1.34375, 'no_speech_prob': 0.05649280548095703}, {'id': 1, 'seek': 0, 'start': 8.0, 'end': 14.0, 'text': ' daily task more efficient when also very concerned about privacy and job replacement.', 'tokens': [50764, 5212, 5633, 544, 7148, 562, 611, 588, 5922, 466, 11427, 293, 1691, 14419, 13, 51064], 'temperature': 0.0, 'avg_logprob': -0.47807203020368305, 'compression_ratio': 1.34375, 'no_speech_prob': 0.05649280548095703}], 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import librosa\n",
    "\n",
    "path_audio = '../uploads/recording.wav'\n",
    "model = whisper.load_model(\"base\")  # Load the Whisper model\n",
    "result = model.transcribe(path_audio, language='en', beam_size=5, best_of=3)  # Transcribe the audio\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) no_speech_prob\n",
    "- Logits: Raw scores from the model for different classes (e.g., speech, no speech).\n",
    "- Softmax Function: Converts logits into probabilities.\n",
    "- No Speech Probability: The probability for the \"no speech\" class after applying softmax.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{no\\_speech\\_prob} = \\sum_{j} e^{z_j} / e^{z_{\\text{no\\_speech}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /Users/ngocp/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the CMU Pronouncing Dictionary: 123455\n",
      "Pronunciation of 'hello': [['HH', 'AH0', 'L', 'OW1'], ['HH', 'EH0', 'L', 'OW1']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Load the CMU Pronouncing Dictionary\n",
    "d = cmudict.dict()\n",
    "print(f\"Number of words in the CMU Pronouncing Dictionary: {len(d)}\")\n",
    "print(f\"Pronunciation of 'hello': {d['hello']}\")\n",
    "# Get the pronunciation of a word\n",
    "word = \"hello\"\n",
    "# pronunciation = d[word.lower()][0]\n",
    "# print(pronunciation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_phonetic_transcription(word):\n",
    "    \"\"\"Returns the phonetic transcription of a word.\"\"\"\n",
    "    return d.get(word.lower())\n",
    "\n",
    "def syllable_to_char_map(word, syllables):\n",
    "    \"\"\"Map each syllable to its corresponding characters in the word.\"\"\"\n",
    "    char_map = []\n",
    "    start = 0\n",
    "    for syllable in syllables:\n",
    "        end = start + len(syllable)\n",
    "        char_map.append(word[start:end])\n",
    "        start = end\n",
    "    return char_map\n",
    "\n",
    "def highlight_syllables(expected_syllables, actual_syllables, word):\n",
    "    \"\"\"Highlight each syllable, red for wrong syllable, green for right syllable.\"\"\"\n",
    "    highlighted = []\n",
    "    char_map = syllable_to_char_map(word, actual_syllables)\n",
    "    for exp_syl, act_syl, chars in zip(expected_syllables, actual_syllables, char_map):\n",
    "        print(f\"highlight_syllables: {exp_syl} -> {act_syl} -> {chars}\")\n",
    "        if exp_syl == act_syl:\n",
    "            highlighted.append(f'<span style=\"color: green;\">{chars}</span>')\n",
    "        else:\n",
    "            highlighted.append(f'<span style=\"color: red;\">{chars}</span>')\n",
    "    return highlighted\n",
    "\n",
    "def compare_phonetic_transcriptions(expected, actual):\n",
    "    \"\"\"Compare the phonetic transcriptions of the expected and actual words.\"\"\"\n",
    "    feedback = []\n",
    "    for exp_word, act_word in zip(expected.split(), actual.split()):\n",
    "        exp_phonetic = get_phonetic_transcription(exp_word)\n",
    "        act_phonetic = get_phonetic_transcription(act_word)\n",
    "        print(f\"Expected: {exp_word} -> {exp_phonetic} -> {act_word}\")\n",
    "        if exp_phonetic and act_phonetic:\n",
    "            exp_syllables = [syllable for phoneme in exp_phonetic for syllable in phoneme]\n",
    "            act_syllables = [syllable for phoneme in act_phonetic for syllable in phoneme]\n",
    "            highlighted_syllables = highlight_syllables(exp_syllables, act_syllables, act_word)\n",
    "            feedback.append({'word': act_word, 'highlighted_syllables': ''.join(highlighted_syllables)})\n",
    "        else:\n",
    "            feedback.append({'word': act_word, 'highlighted_syllables': f'<span style=\"color: red;\">{act_word}</span>'})\n",
    "    \n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: We -> [['W', 'IY1']] -> We\n",
      "highlight_syllables: W -> W -> W\n",
      "highlight_syllables: IY1 -> IY1 -> e\n",
      "Expected: are -> [['AA1', 'R'], ['ER0']] -> ought\n",
      "highlight_syllables: AA1 -> AO1 -> oug\n",
      "highlight_syllables: R -> T -> h\n",
      "Expected: to -> [['T', 'UW1'], ['T', 'IH0'], ['T', 'AH0']] -> to\n",
      "highlight_syllables: T -> T -> t\n",
      "highlight_syllables: UW1 -> UW1 -> o\n",
      "highlight_syllables: T -> T -> \n",
      "highlight_syllables: IH0 -> IH0 -> \n",
      "highlight_syllables: T -> T -> \n",
      "highlight_syllables: AH0 -> AH0 -> \n",
      "Expected: establish -> [['IH0', 'S', 'T', 'AE1', 'B', 'L', 'IH0', 'SH'], ['IY0', 'S', 'T', 'AE1', 'B', 'L', 'IH0', 'SH']] -> establish\n",
      "highlight_syllables: IH0 -> IH0 -> est\n",
      "highlight_syllables: S -> S -> a\n",
      "highlight_syllables: T -> T -> b\n",
      "highlight_syllables: AE1 -> AE1 -> lis\n",
      "highlight_syllables: B -> B -> h\n",
      "highlight_syllables: L -> L -> \n",
      "highlight_syllables: IH0 -> IH0 -> \n",
      "highlight_syllables: SH -> SH -> \n",
      "highlight_syllables: IY0 -> IY0 -> \n",
      "highlight_syllables: S -> S -> \n",
      "highlight_syllables: T -> T -> \n",
      "highlight_syllables: AE1 -> AE1 -> \n",
      "highlight_syllables: B -> B -> \n",
      "highlight_syllables: L -> L -> \n",
      "highlight_syllables: IH0 -> IH0 -> \n",
      "highlight_syllables: SH -> SH -> \n",
      "Expected: an -> [['AE1', 'N'], ['AH0', 'N']] -> an\n",
      "highlight_syllables: AE1 -> AE1 -> an\n",
      "highlight_syllables: N -> N -> \n",
      "highlight_syllables: AH0 -> AH0 -> \n",
      "highlight_syllables: N -> N -> \n",
      "Expected: environment, -> None -> environment\n",
      "Expected: just -> [['JH', 'AH1', 'S', 'T'], ['JH', 'IH0', 'S', 'T']] -> that\n",
      "highlight_syllables: JH -> DH -> th\n",
      "highlight_syllables: AH1 -> AE1 -> at\n",
      "highlight_syllables: S -> T -> \n",
      "highlight_syllables: T -> DH -> \n",
      "highlight_syllables: JH -> AH0 -> \n",
      "highlight_syllables: IH0 -> T -> \n",
      "Expected: support -> [['S', 'AH0', 'P', 'AO1', 'R', 'T']] -> supports\n",
      "highlight_syllables: S -> S -> s\n",
      "highlight_syllables: AH0 -> AH0 -> upp\n",
      "highlight_syllables: P -> P -> o\n",
      "highlight_syllables: AO1 -> AO1 -> rts\n",
      "highlight_syllables: R -> R -> \n",
      "highlight_syllables: T -> T -> \n",
      "Expected: sustainable -> [['S', 'AH0', 'S', 'T', 'EY1', 'N', 'AH0', 'B', 'AH0', 'L']] -> sustainable\n",
      "highlight_syllables: S -> S -> s\n",
      "highlight_syllables: AH0 -> AH0 -> ust\n",
      "highlight_syllables: S -> S -> a\n",
      "highlight_syllables: T -> T -> i\n",
      "highlight_syllables: EY1 -> EY1 -> nab\n",
      "highlight_syllables: N -> N -> l\n",
      "highlight_syllables: AH0 -> AH0 -> e\n",
      "highlight_syllables: B -> B -> \n",
      "highlight_syllables: AH0 -> AH0 -> \n",
      "highlight_syllables: L -> L -> \n",
      "Expected: practices. -> None -> practices.\n",
      "{'word': 'We', 'highlighted_syllables': '<span style=\"color: green;\">W</span><span style=\"color: green;\">e</span>'}\n",
      "{'word': 'ought', 'highlighted_syllables': '<span style=\"color: red;\">oug</span><span style=\"color: red;\">h</span>'}\n",
      "{'word': 'to', 'highlighted_syllables': '<span style=\"color: green;\">t</span><span style=\"color: green;\">o</span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span>'}\n",
      "{'word': 'establish', 'highlighted_syllables': '<span style=\"color: green;\">est</span><span style=\"color: green;\">a</span><span style=\"color: green;\">b</span><span style=\"color: green;\">lis</span><span style=\"color: green;\">h</span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span>'}\n",
      "{'word': 'an', 'highlighted_syllables': '<span style=\"color: green;\">an</span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span>'}\n",
      "{'word': 'environment', 'highlighted_syllables': '<span style=\"color: red;\">environment</span>'}\n",
      "{'word': 'that', 'highlighted_syllables': '<span style=\"color: red;\">th</span><span style=\"color: red;\">at</span><span style=\"color: red;\"></span><span style=\"color: red;\"></span><span style=\"color: red;\"></span><span style=\"color: red;\"></span>'}\n",
      "{'word': 'supports', 'highlighted_syllables': '<span style=\"color: green;\">s</span><span style=\"color: green;\">upp</span><span style=\"color: green;\">o</span><span style=\"color: green;\">rts</span><span style=\"color: green;\"></span><span style=\"color: green;\"></span>'}\n",
      "{'word': 'sustainable', 'highlighted_syllables': '<span style=\"color: green;\">s</span><span style=\"color: green;\">ust</span><span style=\"color: green;\">a</span><span style=\"color: green;\">i</span><span style=\"color: green;\">nab</span><span style=\"color: green;\">l</span><span style=\"color: green;\">e</span><span style=\"color: green;\"></span><span style=\"color: green;\"></span><span style=\"color: green;\"></span>'}\n",
      "{'word': 'practices.', 'highlighted_syllables': '<span style=\"color: red;\">practices.</span>'}\n"
     ]
    }
   ],
   "source": [
    "user_text = \"We are to establish an environment, just support sustainable practices.\"\n",
    "reference_text = \"We ought to establish an environment that supports sustainable practices.\"\n",
    "\n",
    "feedback = compare_phonetic_transcriptions(user_text, reference_text)\n",
    "\n",
    "for item in feedback:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    \"\"\"Returns the number of syllables in a word.\"\"\"\n",
    "    if word.lower() in d:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    else:\n",
    "        return len([char for char in word if char in 'aeiouy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_ipa_transcription(text):\n",
    "    result = subprocess.run(\n",
    "        ['espeak-ng', '-x', text, '--ipa'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    return result.stdout.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def compare_phonetic_sequences(seq1, seq2):\n",
    "    # Convert IPA sequences to numerical representation if needed\n",
    "    # For simplicity, this example uses string comparison\n",
    "    distance, path = fastdtw(seq1, seq2, dist=euclidean)\n",
    "    return distance, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_transcription:  secret is simple sentence.\n",
      "Expected phonetic transcription: həlˈəʊ\n",
      "hˈaʊ ɑː juː sˈiːkɹɪt ɪz sˈɪmpəl sˈɛntəns\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'sˈiːkɹɪt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m user_phonetic \u001b[38;5;241m=\u001b[39m get_ipa_transcription(user_transcription)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected phonetic transcription:\u001b[39m\u001b[38;5;124m\"\u001b[39m, expected_phonetic, user_phonetic)\n\u001b[0;32m---> 20\u001b[0m distance, path \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_phonetic_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_phonetic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_phonetic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m discrepancies \u001b[38;5;241m=\u001b[39m highlight_discrepancies(user_transcription, user_phonetic\u001b[38;5;241m.\u001b[39msplit(), expected_phonetic\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiscrepancies:\u001b[39m\u001b[38;5;124m\"\u001b[39m, discrepancies)\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mcompare_phonetic_sequences\u001b[0;34m(seq1, seq2)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompare_phonetic_sequences\u001b[39m(seq1, seq2):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Convert IPA sequences to numerical representation if needed\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# For simplicity, this example uses string comparison\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     distance, path \u001b[38;5;241m=\u001b[39m \u001b[43mfastdtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meuclidean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m distance, path\n",
      "File \u001b[0;32mfastdtw/_fastdtw.pyx:78\u001b[0m, in \u001b[0;36mfastdtw._fastdtw.fastdtw\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfastdtw/_fastdtw.pyx:222\u001b[0m, in \u001b[0;36mfastdtw._fastdtw.__prep_inputs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'sˈiːkɹɪt'"
     ]
    }
   ],
   "source": [
    "def highlight_discrepancies(user_text, user_phonetic, expected_phonetic):\n",
    "    discrepancies = []\n",
    "    for i, (user, expected) in enumerate(zip(user_phonetic, expected_phonetic)):\n",
    "        if user != expected:\n",
    "            discrepancies.append((i, user, expected))\n",
    "    return discrepancies\n",
    "\n",
    "# Example usage\n",
    "user_text = \"Hello, how are you?\"\n",
    "expected_text = \"Hello, how are you?\"\n",
    "\n",
    "path_user_voice = \"../uploads/recording.wav\"\n",
    "user_transcription = transcribe_audio(path_user_voice)\n",
    "print('user_transcription:', user_transcription)\n",
    "\n",
    "expected_phonetic = get_ipa_transcription(expected_text)\n",
    "user_phonetic = get_ipa_transcription(user_transcription)\n",
    "\n",
    "print(\"Expected phonetic transcription:\", expected_phonetic, user_phonetic)\n",
    "distance, path = compare_phonetic_sequences(user_phonetic.split(), expected_phonetic.split())\n",
    "discrepancies = highlight_discrepancies(user_transcription, user_phonetic.split(), expected_phonetic.split())\n",
    "\n",
    "print(\"Discrepancies:\", discrepancies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
